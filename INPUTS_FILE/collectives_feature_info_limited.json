{
  "name": "PyTorch Collective Operations",
  "description": "Collective operations in PyTorch are distributed communication primitives that enable efficient data exchange between processes in a distributed setting. They're essential for parallel and distributed training of deep learning models, allowing processes to coordinate and share information efficiently.",
  "api": [
    "torch.distributed.all_reduce"
    ],
  "examples": [
    "# All-reduce example (sum operation)\nimport torch.distributed as dist\ndist.init_process_group(backend='gloo')\ntensor = torch.ones(10)\ndist.all_reduce(tensor)\n# After this, all processes have tensor with value equal to world_size"
  ],
  "requirements": {
    "test_requirements": [
        "For every collective generate functional tests. \n There should be 1 implementation file for each collective and all types of tests should be in it. "
    ],
    "functional_requirements": [
      "All collective operations should work correctly with various tensor shapes and sizes",
      "Tests should support different data types (float32, float16)",
      "Tests should work with both CPU and GPU tensors",
      "Tests should handle edge cases like empty tensors and single-element tensors",
      "Operations should properly synchronize processes"
    ]
  }
}