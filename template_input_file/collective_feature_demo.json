{
  "name": "PyTorch Collective Operations",
  "description": "Collective operations in PyTorch are distributed communication primitives that enable efficient data exchange between processes in a distributed setting. They're essential for parallel and distributed training of deep learning models, allowing processes to coordinate and share information efficiently. Generate test plan for the following collectives API: torch.distributed.all_reduce. For every collective generate only functional tests for now. \n There should be 1 implementation file for each collective and all types of tests should be in it. Parametrize the tests for float32, float16 data types, medium size tensor, with 2d and 3d tensor dims and fixed world size of 4. Tests should work with HPU tensors."
}
