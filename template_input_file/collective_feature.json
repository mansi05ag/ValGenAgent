{
  "name": "PyTorch Collective Operations",
  "description": "Collective operations in PyTorch are distributed communication primitives that enable efficient data exchange between processes in a distributed setting. They're essential for parallel and distributed training of deep learning models, allowing processes to coordinate and share information efficiently. Generate test plan for the following collectives API: torch.distributed.all_reduce. For every collective generate 1. tests that check performance using torch_profiler 2. edge case tests  3. functional tests 4. zero size tensor tests 5. big tensor tests. \n There should be 1 implementation file for each collective and all types of tests should be in it. Parametrize the tests for various data types(float32, float16, int64, etc.), tensor shapes and world size. Tests should work with CPU, GPU and HPU tensors. Tests should handle edge cases like empty tensors and single-element tensors. Operations should properly synchronize processes. Operations should scale efficiently with increasing world size. Operations should minimize memory overhead. Operations should properly utilize available bandwidth."
}
